{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reconocimiento de Patrones - Clasificacion de Imagenes - Resnet101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiagomvc/reconocimiento-patrones-unal/blob/master/Clasificacion_de_Imagenes_Resnet101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAQytaIKk_si",
        "colab_type": "text"
      },
      "source": [
        "# Trabajo Final Reconocimiento de Patrones - Clasificación de Imagenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfrvS1cgTBFZ",
        "colab_type": "text"
      },
      "source": [
        "# **!!!Es recomendado correr este Notebook en Google Colab, de otra forma seran necesarios cambios para su correcta ejecución!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orucP_elG5_",
        "colab_type": "text"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "El objetivo principal de este desarrollo es aplicar los conceptos vistos en la materia Reconocimiento de Patrones  y en demas materias del posgrado en Analítica de la Universidad Nacional de Colombia Sede Medellín. Con este fin se desarrollo un sistema de clasificación de señales de transito con las siguientes características:\n",
        "\n",
        "Objetivo General\n",
        "\n",
        "*   Generar un Sistema de Clasificacion de Señales de transito basado en fotos reales recortadas de las señales.\n",
        "\n",
        "\n",
        "Objetivos Específicos\n",
        "\n",
        "*   Entrenar un algoritmo que permita de forma precisa (>90% Accuracy en development set) clasificar fotos de señales de transito (con distribuciones similares a la data de entrenamiento) en sus respectivas categorías. \n",
        "*   Crear una Inferfaz de usuario amigable para el uso interactivo del algoritmo que permita su entendimiento y validacion. \n",
        "\n",
        "\n",
        "Este tipo de sistemas, en conjunto con algoritmos adicionales de Computer Vision, pueden tener múltiples usos, entre los que se encuentran:\n",
        "\n",
        "\n",
        "*   Evaluacion de señales por parte de vehículos autonomos para la toma de decisiones en la vía\n",
        "*   Auditoría gubernamental de la cantidad y ubicación de las diferentes señales de transito en un área determinada  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPQfj8UtlS3g",
        "colab_type": "text"
      },
      "source": [
        "## Metodología\n",
        "\n",
        "\n",
        "Para el desarrollo del sistema de clasificación se decidió seguir la metodologia de proyectos de analítica CRISPDM, la cual consiste de 6 pasos\n",
        "\n",
        "![Metodología CRISPDM](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/800px-CRISP-DM_Process_Diagram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZOdkn7glXPa",
        "colab_type": "text"
      },
      "source": [
        "## 1. Entendimiento del Problema\n",
        "\n",
        "Con los avances técnicos de los últimos años en Vision por Computador (CV) y el aumento en la capacidad de almacenamiento de data no estructurada, han aumentado de forma significativa el número de aplicaciones industriales que requieren encontrar patrones en imagenes.\n",
        "\n",
        "Dentro de estas aplicaciones se encuentra la clasificación de imagenes, que permite inferir la clase de un objeto con base en una imagen que lo represente.\n",
        "La clasificacion de señales de transito, en conjunto con algoritmos de detección de objetos que permita identificar donde existen estas señales,  presenta múltiples aplicaciones incluyendo reconocimiento de señales para toma de decision de automóviles autonomos y auditoría de señales de transito por entes gubernamentales.\n",
        "\n",
        "En este ejercicio nos concentraremos en la aplicación para la auditoría de señales, por lo que sera de ayuda una herramienta interactiva que permita comprender y validar el funcionamiento del modelo. \n",
        "\n",
        "![Traffic Sign Detection](https://www.itsinternational.com/_resources/assets/inline/custom/72/131413.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeMLLBQildFb",
        "colab_type": "text"
      },
      "source": [
        "## 2. Entendimiento de la Data\n",
        "\n",
        "Para la realización de este sistema y el entrenamiento del modelo se utilizó un dataset libre al público, el cual se puede descargar en el siguiente link: https://www.kaggle.com/c/ml-medellin-mar2019/data.\n",
        "\n",
        "El dataset consiste de 27439 fotografías de señales de transito a color de 64x64 pixeles, cada una correspondiente a una categoría asociada al nombre del directorio que las almacena. En total existen 43 categorías posibles, que van desde señales de pare hasta límites de velocidad.\n",
        "\n",
        "Es importante señalar que las imagenes utilizadas para el entrenamiento del modelo estan recortadas para incluir principalmente la información de las señales, por lo que para la aplicación en producción del sistema las imagenes de entrada deben ser preprocesadas manual o automáticamente para extraer las porciones que incluyan algún tipo de señal.\n",
        "\n",
        "![Límite 20](https://i.ibb.co/jgfMdMG/27943933934972.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21NKDcIjUVN4",
        "colab_type": "text"
      },
      "source": [
        "### Para correr en Colab recomendamos:\n",
        "a. Cargar los datos preprocesados\n",
        "1.   Descargar el archivo \"ml-medellin-mar2019\" aquí https://drive.google.com/file/d/1BPpBJn_ZwS3M5l-raaX1sd8X1JgJ8ouQ/view?usp=sharing\n",
        "2.   Descomprimir el archivo descargado \n",
        "2.   Subir la carpeta generada a la raiz de Google Drive con el nombre \"ml-medellin-mar2019\"\n",
        "\n",
        "b. Procesar los datos en Colab (Lento)\n",
        "1.   Descargar la data de https://www.kaggle.com/c/ml-medellin-mar2019/data\n",
        "2.   Descomprimir la carpeta descargada\n",
        "3.   Descomprimir los archivos comprimidos dentro de la carpeta\n",
        "4.   Subir la carpeta con los datos a la raiz de Google Drive con el nombre \"ml-medellin-mar2019\"\n",
        "5.   Comentar las líneas no comentadas y descomentar las líneas comentadas en el bloque de codigo que comienza con \"Reading images as vectors\"\n",
        "\n",
        "De no utilizar Colab recomendamos comentar las lineas relacionadas y cambiar los paths donde sea necesario.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thk89ggm96WA",
        "colab_type": "code",
        "outputId": "d063fb5d-0890-42d3-b791-aadcac474ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvnVHyOMaa1L",
        "colab_type": "code",
        "outputId": "dabaa6a0-3058-4b53-89f5-09662f03cae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Downloading not yet ready libraries\n",
        "pip install scipy==1.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkWHY3CwLu0X",
        "colab_type": "code",
        "outputId": "0ed6f21f-92cb-46cb-97f3-7eb1ab4ac83f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "# importing Libraries\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.framework import ops\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from keras import layers, optimizers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx1xGBg19ZOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining paths\n",
        "path = '/content/drive/My Drive/ml-medellin-mar2019'\n",
        "train = path + '/train'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVOP_js49gn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading images as vectors \n",
        "#X = []\n",
        "#Y = []\n",
        "#for i in os.listdir(train):\n",
        "#    label = i\n",
        "#    print(label)\n",
        "#    for j in  os.listdir(train + '/' + i):\n",
        "#        fname = train + '/' + i + '/' + j\n",
        "#        image = np.array(ndimage.imread(fname, flatten=False))\n",
        "#        reshaped_image = scipy.misc.imresize(image, size=(64,64))\n",
        "#        X.append(reshaped_image)\n",
        "#        Y.append(int(label))\n",
        "\n",
        "\n",
        "#X = np.array(X)\n",
        "#Y = np.array(Y)\n",
        "\n",
        "X = np.load(path + '/X.npy')\n",
        "Y = np.load(path + '/Y.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA8L9Q69lgYd",
        "colab_type": "text"
      },
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "Para el adecuado procesamiento de las imagenes y posterior entrenamiento del modelo es necesario almacenar las señales de transito de forma numérica, donde se utilizan tensores para representar los valores de los pixeles de cada imagen. Por tanto, una imagen a color de 64x64 pixeles es representada como un tensor de dimensiones (64,64, 3).\n",
        "De forma mas general, un dataset con m ejemplos, h pixeles de altura, w pixeles de largo y c número de canales se representa con un tensor de la siguiente forma (m,h,w,c).\n",
        "Por su parte, las categorías objetivo Y se representan con una matriz (num_clases, m) que permite comparación con las predicciones del algoritmo y el entrenamiento del modelo\n",
        "\n",
        "Posterior a la representación  de los datos en forma de tensores, se normalizan los valores de los pixeles para facilitar el entrenamiento del modelo, el cual puede verse afectado de forma positiva por la representacion de los valores de entrada alrededor de 0. Para la normalizacion se dividen los valores de cada pixel por 255, que es el valor máximo que pueden presentar.\n",
        "\n",
        "Por último, se procede a la separación del conjunto de datos m en dos conjuntos de datos diferentes, X_train,Y_train para el entrenamiento del modelo y X_test,Y_test para la validación de los resultados sobre un dataset no observado durante el entrenamiento.\n",
        "\n",
        "Dado que la cantidad de datos es significativa, y que algunos algoritmos para modelamiento con imagenes necesitan una gran cantidad de datos, se decidio entrenar con el 95% de los datos y validar con el 5% restante. Es importante agregar que esta data es ordenada de forma aleatoria para que el modelo no aprenda relaciones espurias con base en la ubicación relativa de las imagenes.\n",
        "\n",
        "![Images as Tensors](https://image.slidesharecdn.com/tensordecomposition-170301235239/95/a-brief-survey-of-tensors-5-638.jpg?cb=1488412458)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFR-_ffK-WZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizando los valores\n",
        "X = X/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV5gmQfW_EaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separando dev and test sets\n",
        "dev_size = math.ceil(0.05 * X.shape[0])\n",
        "dev_index = np.random.choice(list(range(0, X.shape[0])), dev_size, replace = False)\n",
        "X_test = X[dev_index, :, :, :]\n",
        "Y_test = Y[dev_index] \n",
        "X_train = np.delete(X, dev_index, axis = 0)\n",
        "Y_train = np.delete(Y, dev_index, axis = 0)\n",
        "\n",
        "shuffled_train = list(range(0, X_train.shape[0]))\n",
        "np.random.shuffle(shuffled_train)\n",
        "X_train = X_train[shuffled_train, :, :, :]\n",
        "Y_train = Y_train[shuffled_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUpm-fAO_Ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Representando las clases\n",
        "lb = LabelBinarizer()\n",
        "Y_train = lb.fit_transform(Y_train)\n",
        "Y_test = lb.fit_transform(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhs6JSU8ll5N",
        "colab_type": "text"
      },
      "source": [
        "## 4. Modelamiento\n",
        "\n",
        "Para modelar las clases de las señales de transito con base en sus fotografías se decidio utilizar un modelo de redes neuronales (NN). Las redes neuronales son conjuntos de uno o mas nodos que representan una transformación lineal de un input A\\[l] dados unos parametros (W\\[l],b\\[l]), seguida de una transformación no lineal del resultado g\\[l] (g\\[l] puede ser Tanh, ReLU,Sigmoid); donde los parametros (W\\[l],b\\[l]) son entrenados por medio del algoritmo de Backpropagation.\n",
        "\n",
        "Debido a la gran cantidad de problemas abordados a través de NN existen múltiples arquitecturas que funcionan mejor dada la naturaleza del problema. Dado su éxito en tareas de modelamiento de imagenes se decidió utilizar redes neuronales convolucionales, las cuales son capaces de aprender filtros (W) capaces de extraer información de valor de las imagenes por medio de convoluciones. \n",
        "\n",
        "Especificamente se decidio utilizar un modelo basado en la arquitectura de Resnet101, la cual tiene más de 100 capas dentro de la red neuronal que se entrenan de forma adecuada gracias a su mecanismo de conexiones de atajo, que permite aprender funciones de identidad para múltiples capas y mejora los resultados de backpropagation. Además, esta arquitectura ha sido utilizada de forma exitosa en competencias como ImageNet y en datasets libres como CIFAR10.\n",
        "\n",
        "Para la red neuronal se utilizaron los siguientes parametros:\n",
        "\n",
        "\n",
        "* Arquitectura base: Resnet 101\n",
        "* Número de capas: 101\n",
        "* Learning Rate: 0.00005\n",
        "* beta_1: 0.9\n",
        "* beta_2: 0.999\n",
        "* decay: 0.00005/25\n",
        "* Optimizer: Adam\n",
        "* Loss Function: categorical_crossentropy\n",
        "* Epochs : 20\n",
        "* Batch Size: 32\n",
        "\n",
        "\n",
        "![Resnets](https://www.d2l.ai/_images/resnet-block.svg)\n",
        "\n",
        "El siguiente codigo fue realizado con base en los ejercios de Deep Learning Specialization, Coursera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkqy1qOULfVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 4\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pJfvMewM_D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = Conv2D(F2, (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### (≈2 lines)\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1' )(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4P9jfDfNEAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ResNet101(input_shape = (64, 64, 3), classes = 6):\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet101 the following architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*22 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3 (≈4 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4 (≈23 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='g')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='h')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='i')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='j')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='k')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='l')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='m')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='n')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='o')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='p')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='q')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='r')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='s')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='t')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='u')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='v')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='w')\n",
        "\n",
        "\n",
        "    # Stage 5 (≈3 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    X = AveragePooling2D((2,2), name = 'avg_pool')(X)\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    #X = Dropout(rate = .8)(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet101')\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwK5GeXiNJNX",
        "colab_type": "code",
        "outputId": "4cc91c41-ff7b-4cd4-f4c6-1239949d2a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "model = ResNet101(input_shape = (64, 64, 3), classes = 43)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tttv7qCnNygy",
        "colab_type": "code",
        "outputId": "8c9fcd34-6f70-48f3-ce8d-3d90172ce393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "adam = optimizers.Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00005/25, amsgrad=False)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVRGsTXMQH2C",
        "colab_type": "code",
        "outputId": "c843abb2-4ae4-4ac5-86b0-1901b58ca0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "source": [
        "model.fit(X_train, Y_train, epochs = 20, batch_size = 32, shuffle = True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/20\n",
            "26067/26067 [==============================] - 142s 5ms/step - loss: 2.8493 - acc: 0.2333\n",
            "Epoch 2/20\n",
            "26067/26067 [==============================] - 122s 5ms/step - loss: 1.3829 - acc: 0.5726\n",
            "Epoch 3/20\n",
            "26067/26067 [==============================] - 122s 5ms/step - loss: 0.6578 - acc: 0.7913\n",
            "Epoch 4/20\n",
            "26067/26067 [==============================] - 122s 5ms/step - loss: 0.3568 - acc: 0.8865\n",
            "Epoch 5/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.2315 - acc: 0.9245\n",
            "Epoch 6/20\n",
            "26067/26067 [==============================] - 122s 5ms/step - loss: 0.1794 - acc: 0.9425\n",
            "Epoch 7/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.1284 - acc: 0.9587\n",
            "Epoch 8/20\n",
            "26067/26067 [==============================] - 120s 5ms/step - loss: 0.1087 - acc: 0.9652\n",
            "Epoch 9/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.1051 - acc: 0.9663\n",
            "Epoch 10/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.0855 - acc: 0.9745\n",
            "Epoch 11/20\n",
            "26067/26067 [==============================] - 120s 5ms/step - loss: 0.0812 - acc: 0.9752\n",
            "Epoch 12/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.0567 - acc: 0.9831\n",
            "Epoch 13/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.0567 - acc: 0.9831\n",
            "Epoch 14/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.0553 - acc: 0.9834\n",
            "Epoch 15/20\n",
            "26067/26067 [==============================] - 120s 5ms/step - loss: 0.0448 - acc: 0.9868\n",
            "Epoch 16/20\n",
            "26067/26067 [==============================] - 120s 5ms/step - loss: 0.0404 - acc: 0.9881\n",
            "Epoch 17/20\n",
            "26067/26067 [==============================] - 122s 5ms/step - loss: 0.0450 - acc: 0.9865\n",
            "Epoch 18/20\n",
            "26067/26067 [==============================] - 122s 5ms/step - loss: 0.0299 - acc: 0.9914\n",
            "Epoch 19/20\n",
            "26067/26067 [==============================] - 122s 5ms/step - loss: 0.0320 - acc: 0.9909\n",
            "Epoch 20/20\n",
            "26067/26067 [==============================] - 121s 5ms/step - loss: 0.0366 - acc: 0.9891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f940c8f1d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZU6eKREl3Yv",
        "colab_type": "text"
      },
      "source": [
        "## 5. Evaluación\n",
        "\n",
        "Para la evaluación del modelo se utilizo la metrica de exactitud, que mide el total de las predicciones correctas sobre el total de las predicciones realizadas.\n",
        "\n",
        "Al evaluar el modelo sobre la data de entrenamiento se encuentra que la exactitud es del 99.26%, mientras que en el dataset de desarrollo la exactitud es del 97.8%, con datos que el modelo no recibió durante el entrenamiento.\n",
        "\n",
        "Por último, también se valido que no existieran saltos anormales en la función de perdida durante el entrenamiento del modelo\n",
        "\n",
        "![Loss Functions](http://cs231n.github.io/assets/nn3/learningrates.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCw_9A_VQ4kh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8625e732-53e6-4c36-cd2f-dcbf11d68e10"
      },
      "source": [
        "preds = model.evaluate(X_test, Y_test)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1372/1372 [==============================] - 3s 2ms/step\n",
            "Loss = 0.09141741659622585\n",
            "Test Accuracy = 0.9759475218658892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQP6if1xRzes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Writing Model\n",
        "model.save(path + '/resnet_traffic_r101.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbOulEromM2r",
        "colab_type": "text"
      },
      "source": [
        "## 6. Despliegue\n",
        "\n",
        "Para el despliegue del sistema en pro de la validación de resultados y entendimiento del modelo por parte de usuarios finales se utilizo la libreria Dash de python, para realizar el despiegue de un tablero de control que permita cargar imagenes en línea de señales de transito e inferir la clase esperada.\n",
        "\n",
        "Para explorar el tablero entrar a:\n",
        "https://trafficdash.azurewebsites.net/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M92mw5sn0Gf4",
        "colab_type": "text"
      },
      "source": [
        "## Bibliografía\n",
        "\n",
        "\n",
        "*   El modelo CRISP-DM: el nuevo plan para la minería de datos, almacenamiento de los datos. Shearer C., (2000); 5:13-22.\n",
        "*   List Gradient-based learning applied to document recognition. Y LeCun, L Bottou, Y Bengio, P Haffner. Proceedings of the IEEE 86 (11), 2278-2324, 1998.\n",
        "*   The Elements of Statistical Learning. T. Hastie, R. Tibshirani, and J. Friedman. Springer Series in Statistics Springer New York Inc., New York, NY, USA, (2001)\n",
        "*   Deep Learning (Ian J. Goodfellow, Yoshua Bengio and Aaron Courville), MIT Press, 2016.\n",
        "*   Deep Learning Specialization. Coursera.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}